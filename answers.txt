1. I dont think it feels like we're p-hacking personally. The results we gathered from the anaylysis is the same as our conlcuding data, we arent making adjustments to any of our anayslysys technqiues or cherry picking from a set of multiple results like we would if we were p-hacking. We defined our Null hypothesis before testing, and presented only the p-values we obtained from the analysis. Although we did know teachers would likely benefit more from the new features before we tested for this, I wouldn't consider it p-hacking either since our anayslys was done in the same manner for the inscturotrs as it was for the whole userbase, we stated our hypothesis knowing this beforehand, and did not change our hyptihesis based on intermdeitate results. Since I dont think we are p-hacking, I am pretty cofnident in our p-value results since the datasets for both tests also seems to be quite large (over 680 users and 235 instrucotrs), but of course we can enever say with 100% certainty that these results are absolute.

2. If we need to conduct t-tests between each sorting algorithm, there would be a total of (7C2) or 21 comparisons. Since our alpha was 0.05, we would have a 5% chance of incorrectl rejecting the Null hypothesis. Since we conduct 21 tests, our proability of getting a false conclusion would be 1-(1-0.05)^21 = 0.659 or almost 66%. This makes our effective alpha value 0.05/21 = 0.00238.

3. Using 